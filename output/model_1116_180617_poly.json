{"model_name": "polynomial_regression", "poly_fea": ["1", "x0", "x1", "x2", "x3", "x0^2", "x0 x1", "x0 x2", "x0 x3", "x1^2", "x1 x2", "x1 x3", "x2^2", "x2 x3", "x3^2", "x0^3", "x0^2 x1", "x0^2 x2", "x0^2 x3", "x0 x1^2", "x0 x1 x2", "x0 x1 x3", "x0 x2^2", "x0 x2 x3", "x0 x3^2", "x1^3", "x1^2 x2", "x1^2 x3", "x1 x2^2", "x1 x2 x3", "x1 x3^2", "x2^3", "x2^2 x3", "x2 x3^2", "x3^3"], "poly_degree": 3, "ols_params": [-29173.541697129447, -6758.031487566885, 58513.749841615325, 118636.93818641124, 60688.764636147665, 14079.836898176174, -38880.38291122258, 47187.2949763175, -10119.188437137054, 13351.953595918305, -282820.0297974625, -16833.337381396355, -39390.00593321207, -273415.70376190345, -16891.68955494075, 8169.258699436257, 4360.410967557145, -137947.9885876457, 36242.57998850802, -27513.62186016272, 242962.44807691005, -57725.642142393684, 34100.63716510689, -18882.470883146103, 556.8814738676301, 4308.029926922165, 11987.177364162984, -16568.05395256306, -377.2611697255343, 300350.16797042266, -46972.503390381724, 5069.890973470203, 23291.929150467215, 166980.45487807138, -16875.60565702166], "reconstruct": "y = dot(poly_fea(X), ols_params) * 128", "note": "The model uses the (Ordinary Least Square)OLS model to fit the best value for polynomial features which generated with scikit-learn(sklearn). The polynomial features are quite simple, sklearn enumerate all combination for input vector with degree D(by default is 2). In here, assume there are 3 dimension in given vector, which denoted to (X0, X1, X2), then sklearn would generate following polynomial features: (1, X0, X1, X2, X0^2, X0*X1, X0*X2, X1^2, X1*X2, X1^2). By optimizing the parameters to find the closest polynomial function could describe the given vectors with the lowest square error compare to observation value y."}