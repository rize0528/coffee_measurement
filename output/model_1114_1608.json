{"model_name": "polynomial_regression", "poly_fea": ["1", "x0", "x1", "x2", "x3", "x0^2", "x0 x1", "x0 x2", "x0 x3", "x1^2", "x1 x2", "x1 x3", "x2^2", "x2 x3", "x3^2", "x0^3", "x0^2 x1", "x0^2 x2", "x0^2 x3", "x0 x1^2", "x0 x1 x2", "x0 x1 x3", "x0 x2^2", "x0 x2 x3", "x0 x3^2", "x1^3", "x1^2 x2", "x1^2 x3", "x1 x2^2", "x1 x2 x3", "x1 x3^2", "x2^3", "x2^2 x3", "x2 x3^2", "x3^3"], "poly_degree": 3, "ols_params": [4605.304953283918, 11799.086339502057, -29316.352970676307, 28334.37649381376, -39712.84657939186, -25473.60498095714, 33957.56938950406, 85842.23573856562, -76218.94592661512, 13468.440109280316, -165896.16711657323, 178946.55629334372, -107837.94556160565, 55875.372174011936, 40127.59628319982, 6902.615178151202, -1856.1851711750787, -63732.498526060954, 71992.16087105629, -14986.14672472536, 93020.04321912606, -105749.37750549702, -63257.197946107844, 893.5799935774994, 32523.751498897152, 723.651763969181, 25424.045408169273, -31106.079341157252, 146184.818714961, -10216.816966238664, -97280.80756882549, 86327.38433524809, -9976.673381956189, -37869.19316274591, -9044.204791411554], "reconstruct": "y = dot(poly_fea(X), ols_params) * 128", "note": "The model uses the (Ordinary Least Square)OLS model to fit the best value for polynomial features which generated with scikit-learn(sklearn). The polynomial features are quite simple, sklearn enumerate all combination for input vector with degree D(by default is 2). In here, assume there are 3 dimension in given vector, which denoted to (X0, X1, X2), then sklearn would generate following polynomial features: (1, X0, X1, X2, X0^2, X0*X1, X0*X2, X1^2, X1*X2, X1^2). By optimizing the parameters to find the closest polynomial function could describe the given vectors with the lowest square error compare to observation value y."}