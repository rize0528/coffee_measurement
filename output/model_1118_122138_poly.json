{"model_name": "polynomial_regression", "poly_fea": ["1", "x0", "x1", "x2", "x0^2", "x0 x1", "x0 x2", "x1^2", "x1 x2", "x2^2", "x0^3", "x0^2 x1", "x0^2 x2", "x0 x1^2", "x0 x1 x2", "x0 x2^2", "x1^3", "x1^2 x2", "x1 x2^2", "x2^3"], "poly_degree": 3, "ols_params": [29544.331034287112, -2751.7262130407616, 5755.309408035362, -258755.18143087067, -5837.561786284095, 19078.02202318801, 16635.39922916703, -12360.225633932394, -39322.04256870039, 757172.5035700053, 3197.4990435311774, -8113.67914480179, 11580.21054288432, 9189.212838749729, -49522.44406406174, -21078.068308763206, -4085.19191630335, 33623.38787097286, 64060.32048769295, -741370.3140560612], "reconstruct": "y = dot(poly_fea(X), ols_params) * 128", "note": "The model uses the (Ordinary Least Square)OLS model to fit the best value for polynomial features which generated with scikit-learn(sklearn). The polynomial features are quite simple, sklearn enumerate all combination for input vector with degree D(by default is 2). In here, assume there are 3 dimension in given vector, which denoted to (X0, X1, X2), then sklearn would generate following polynomial features: (1, X0, X1, X2, X0^2, X0*X1, X0*X2, X1^2, X1*X2, X1^2). By optimizing the parameters to find the closest polynomial function could describe the given vectors with the lowest square error compare to observation value y."}